{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from raco.backends.spark.connection import SparkConnection\n",
    "from raco.backends.spark.catalog import SparkCatalog\n",
    "from raco.backends.spark.algebra import SparkAlgebra\n",
    "from raco.backends.myria.connection import MyriaConnection\n",
    "from raco.backends.myria.catalog import MyriaCatalog\n",
    "from raco.backends.myria import MyriaLeftDeepTreeAlgebra\n",
    "from raco.backends.federated.connection import FederatedConnection\n",
    "from raco.backends.federated.catalog import FederatedCatalog\n",
    "from raco.backends.federated import FederatedAlgebra\n",
    "from raco.backends.federated.algebra import FederatedExec\n",
    "\n",
    "import raco.myrial.interpreter as interpreter\n",
    "import raco.myrial.parser as myrialparser\n",
    "from optparse import OptionParser\n",
    "\n",
    "import raco.viz\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_myria_connection():\n",
    "    rest_url = 'https://rest.myria.cs.washington.edu:1776'\n",
    "    execution_url = 'http://demo.myria.cs.washington.edu'\n",
    "    connection = MyriaConnection(rest_url=rest_url,\n",
    "                                 execution_url=execution_url)\n",
    "    return connection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_spark_connection():\n",
    "    connection = SparkConnection('local')\n",
    "    return connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def query_spark(myriaconnection, sparkconnection, program):\n",
    "    myrial_code = program\n",
    "\n",
    "    myriacatalog = MyriaCatalog(myriaconnection)\n",
    "    sparkcatalog = SparkCatalog(sparkconnection)\n",
    "\n",
    "    catalog = FederatedCatalog([myriacatalog, sparkcatalog])\n",
    "    parser = myrialparser.Parser()\n",
    "    processor = interpreter.StatementProcessor(catalog, True)\n",
    "    statement_list = parser.parse(myrial_code)\n",
    "    processor.evaluate(statement_list)\n",
    "\n",
    "    algebras = [MyriaLeftDeepTreeAlgebra(), SparkAlgebra()]\n",
    "    falg = FederatedAlgebra(algebras, catalog)\n",
    "    \n",
    "    pd = processor.get_physical_plan(target_alg=falg)\n",
    "    sparkconnection.execute_query(pd.args[0].plan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "program_test_project_dimension =\"\"\"\n",
    "abc = scan('/users/shrainik/downloads/sample.dat');\n",
    "b = [from abc where abc.i>5 emit i, j ];\n",
    "store(b, '/users/shrainik/downloads/sample-out.dat');\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local) created by __init__ at raco/backends/spark/connection.py:32 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-f9578b3ad26e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmyriaconn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_myria_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msparkconn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_spark_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-a38116dad208>\u001b[0m in \u001b[0;36mget_spark_connection\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_spark_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkConnection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'local'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/shrainik/Dropbox/raco/raco/backends/spark/connection.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, url, username, password)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \"\"\"\n\u001b[1;32m     31\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'local'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddPyFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/Users/shrainik/Dropbox/raco/raco/backends/spark/pyspark_csv.py'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mimport\u001b[0m \u001b[0mpyspark_csv\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpycsv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/shrainik/Downloads/spark-1.6.0-bin-hadoop2.6/python/pyspark/context.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \"\"\"\n\u001b[1;32m    111\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callsite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfirst_spark_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mCallSite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[0;32m/Users/shrainik/Downloads/spark-1.6.0-bin-hadoop2.6/python/pyspark/context.pyc\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway)\u001b[0m\n\u001b[1;32m    259\u001b[0m                         \u001b[0;34m\" created by %s at %s:%s \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[0;32m--> 261\u001b[0;31m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[1;32m    262\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local) created by __init__ at raco/backends/spark/connection.py:32 "
     ]
    }
   ],
   "source": [
    "myriaconn = get_myria_connection()\n",
    "sparkconn = get_spark_connection()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('i', 'LONG_TYPE'), ('j', 'LONG_TYPE'), ('value', 'LONG_TYPE')]\n",
      "[('i', 'LONG_TYPE'), ('j', 'LONG_TYPE'), ('value', 'LONG_TYPE')]\n",
      "AFTER Spark RULES\n",
      "SparkStore(public:adhoc:/users/shrainik/downloads/sample-out.dat)[SparkProject(i,j)[SparkSelect((i > 5))[SparkScan(public:adhoc:/users/shrainik/downloads/sample.dat)]]]\n",
      "dot version after spark rules\n",
      "digraph G {\n",
      "      ratio = \"fill\" ;\n",
      "      size = \"4.0, 4.0\" ;\n",
      "      page = \"4, 4\" ;\n",
      "      margin = 0 ;\n",
      "      mincross = 2.0 ;\n",
      "      rankdir = \"BT\" ;\n",
      "      nodesep = 0.25 ;\n",
      "      ranksep = 0.25 ;\n",
      "      node [fontname=\"Helvetica\", fontsize=10,\n",
      "            shape=oval, style=filled, fillcolor=white ] ;\n",
      "\n",
      "      // The nodes\n",
      "      \"4509900112\" [label=\"SparkStore(public:adhoc:/users/shrainik/downloads/sample-out.dat)\"] ;\n",
      "      \"4509898960\" [label=\"SparkProject(i,j)\"] ;\n",
      "      \"4509898768\" [label=\"SparkSelect((i > 5))\"] ;\n",
      "      \"4509899984\" [label=\"SparkScan(public:adhoc:/users/shrainik/downloads/sample.dat)\"] ;\n",
      "\n",
      "      // The edges\n",
      "      \"4509898960\" -> \"4509900112\" ;\n",
      "      \"4509898768\" -> \"4509898960\" ;\n",
      "      \"4509899984\" -> \"4509898768\" ;\n",
      "\n",
      "      // The title\n",
      "      labelloc=\"t\";\n",
      "      label=\"\";\n",
      "}\n",
      "+---+---+\n",
      "|  i|  j|\n",
      "+---+---+\n",
      "|  6|302|\n",
      "|  6|442|\n",
      "|  6|504|\n",
      "|  6|649|\n",
      "|  6|672|\n",
      "|  6|725|\n",
      "|  6|738|\n",
      "|  6|876|\n",
      "|  6|909|\n",
      "|  6|921|\n",
      "|  7|520|\n",
      "|  7|530|\n",
      "|  7|554|\n",
      "|  7|792|\n",
      "|  7|806|\n",
      "|  7|838|\n",
      "|  7|864|\n",
      "|  7|879|\n",
      "|  7|992|\n",
      "|  7|997|\n",
      "|  8| 26|\n",
      "|  8|365|\n",
      "|  8|473|\n",
      "|  8|759|\n",
      "|  8|796|\n",
      "|  8|878|\n",
      "|  8|885|\n",
      "|  8|897|\n",
      "|  8|954|\n",
      "|  8|990|\n",
      "|  9|383|\n",
      "|  9|443|\n",
      "|  9|719|\n",
      "|  9|758|\n",
      "|  9|780|\n",
      "|  9|795|\n",
      "|  9|861|\n",
      "|  9|925|\n",
      "|  9|955|\n",
      "|  9|975|\n",
      "| 10|165|\n",
      "| 10|208|\n",
      "| 10|282|\n",
      "| 10|351|\n",
      "| 10|370|\n",
      "| 10|647|\n",
      "| 10|713|\n",
      "| 10|719|\n",
      "| 10|743|\n",
      "| 10|914|\n",
      "| 11| 15|\n",
      "| 11| 22|\n",
      "| 11| 62|\n",
      "| 11|305|\n",
      "| 11|438|\n",
      "| 11|723|\n",
      "| 11|741|\n",
      "| 11|748|\n",
      "| 11|923|\n",
      "| 11|946|\n",
      "| 12| 13|\n",
      "| 12| 14|\n",
      "| 12|158|\n",
      "| 12|320|\n",
      "| 12|531|\n",
      "| 12|578|\n",
      "| 12|612|\n",
      "| 12|655|\n",
      "| 12|791|\n",
      "| 12|978|\n",
      "| 13| 12|\n",
      "| 13|179|\n",
      "| 13|405|\n",
      "| 13|635|\n",
      "| 13|653|\n",
      "| 13|729|\n",
      "| 13|863|\n",
      "| 13|864|\n",
      "| 13|975|\n",
      "| 13|981|\n",
      "| 14| 12|\n",
      "| 14|263|\n",
      "| 14|466|\n",
      "| 14|607|\n",
      "| 14|760|\n",
      "| 14|812|\n",
      "| 14|862|\n",
      "| 14|883|\n",
      "| 14|950|\n",
      "| 14|985|\n",
      "| 15| 11|\n",
      "| 15| 71|\n",
      "| 15|140|\n",
      "| 15|292|\n",
      "| 15|710|\n",
      "| 15|784|\n",
      "| 15|897|\n",
      "| 15|927|\n",
      "| 15|946|\n",
      "| 15|950|\n",
      "+---+---+\n",
      "only showing top 100 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query_spark(myriaconn, sparkconn, program_test_project_dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
